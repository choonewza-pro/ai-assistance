{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1a6c2926-ce33-45e2-ba5d-ed8a78738458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch accelerate\n",
    "# !pip install -U langchain langchain_huggingface langchain-chroma langchain_community sentence-transformers langchain_huggingface langchain_core chromadb ipywidgets pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a8eda47-d029-4a7c-8e3f-ef7b567a36a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /opt/conda/lib/python3.11/site-packages (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "088d1355-6e80-4120-8175-dff161384190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import requests\n",
    "import shutil \n",
    "import time\n",
    "from typing import List\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "class RagUtils:\n",
    "    def __init__(self,embeddings_dir: str):\n",
    "        self.embeddings_dir = embeddings_dir\n",
    "        self.original_get = requests.get\n",
    "        self.model_name = \"BAAI/bge-m3\"\n",
    "        self.model_kwargs = {'device': 'cpu'}\n",
    "        self.encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "    def patched_get(self, url, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Patch the requests.get method to use a custom User-Agent header.\n",
    "        \"\"\"\n",
    "        user_agent =  \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\"\n",
    "        os.environ[\"USER_AGENT\"] =user_agent\n",
    "        headers = kwargs.pop(\"headers\", {})\n",
    "        headers[\"User-Agent\"] = user_agent\n",
    "        return self.original_get(url, headers=headers, *args, **kwargs)\n",
    "\n",
    "    def textSplitter(self, text: str, chunk_size: int, chunk_overlap: int, metadataSource: str):\n",
    "        \"\"\"\n",
    "        Split the input text into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "        \"\"\"\n",
    "        documents = [Document(page_content=text, metadata={\"source\": metadataSource})]\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            strip_whitespace=True,\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Split text into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "\n",
    "    def documentsSplitter(self, documents: Document, chunk_size: int, chunk_overlap: int):\n",
    "        \"\"\"\n",
    "        Split the input documents into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "        \"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            strip_whitespace=True,\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        print(f\"Split text into {len(chunks)} chunks.\")\n",
    "        return chunks\n",
    "\n",
    "    def ingest(self, chunks: List[Document]):\n",
    "        \"\"\"\n",
    "        Ingest the list of Document chunks into a vector database.\n",
    "        \"\"\"\n",
    "        embedding = HuggingFaceEmbeddings(\n",
    "            model_name=self.model_name,\n",
    "            model_kwargs=self.model_kwargs,\n",
    "            encode_kwargs=self.encode_kwargs\n",
    "        )\n",
    "\n",
    "        vector_store = None  # กำหนดค่าเริ่มต้นให้กับตัวแปร vector_store\n",
    "        if os.path.exists(os.path.join(self.embeddings_dir, \"chroma.sqlite3\")):\n",
    "            print(\"-- use Append data to vector store--\")\n",
    "            vector_store = Chroma(persist_directory=self.embeddings_dir, embedding_function=embedding)\n",
    "            vector_store.add_documents(documents=chunks)\n",
    "        else:\n",
    "            print(\"-- use New vector store--\")\n",
    "            os.makedirs(self.embeddings_dir, exist_ok=True)\n",
    "            vector_store = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=self.embeddings_dir)\n",
    "\n",
    "        print(\"-- Ingest to Vector Database Success ---\")\n",
    "        return vector_store\n",
    "\n",
    "    def deleteVectorDatabase(self):\n",
    "        \"\"\"\n",
    "        ลบข้อมูลเก่าในไดเรกทอรี embeddings\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.embeddings_dir):\n",
    "            shutil.rmtree(self.embeddings_dir)\n",
    "            print(f\"Vector database at {self.embeddings_dir} deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Vector database at {self.embeddings_dir} does not exist.\")\n",
    "\n",
    "    def loadContentFromWebsite(self, url: str, targetClassName: str):\n",
    "        \"\"\"\n",
    "        Load content from a website using the specified target class name.\n",
    "        \"\"\"\n",
    "        requests.get = self.patched_get\n",
    "        bs4_strainer = bs4.SoupStrainer(class_=targetClassName)\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(url,),\n",
    "            bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        requests.get = self.original_get\n",
    "        return docs\n",
    "\n",
    "    def loadVectorStore(self):\n",
    "        \"\"\"\n",
    "        Load the vector store from the specified embeddings directory.\n",
    "        \"\"\"\n",
    "        \n",
    "        embedding = HuggingFaceEmbeddings(\n",
    "            model_name=self.model_name,\n",
    "            model_kwargs=self.model_kwargs,\n",
    "            encode_kwargs=self.encode_kwargs\n",
    "        )\n",
    "        vector_store = Chroma(persist_directory=self.embeddings_dir, embedding_function=embedding)\n",
    "        print(\"-- Vector Store Loaded --\")\n",
    "        return vector_store\n",
    "\n",
    "    def deleteDocumentByIds(self, document_ids: List[str]):\n",
    "        \"\"\"\n",
    "        ลบเอกสารใน Chroma vector store ตาม document_ids ที่กำหนด\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(self.embeddings_dir, \"chroma.sqlite3\")):\n",
    "            vector_store = Chroma(persist_directory=self.embeddings_dir, embedding_function=HuggingFaceEmbeddings(\n",
    "                model_name=self.model_name,\n",
    "                model_kwargs=self.model_kwargs,\n",
    "                encode_kwargs=self.encode_kwargs\n",
    "            ))\n",
    "            vector_store.delete(document_ids=document_ids)\n",
    "            print(f\"Documents with IDs {document_ids} deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Vector store at {self.embeddings_dir} does not exist.\")\n",
    "\n",
    "    def deleteAllDocuments(self):\n",
    "        \"\"\"\n",
    "        ลบเอกสารทั้งหมดใน Chroma vector store\n",
    "        \"\"\"\n",
    "        if os.path.exists(os.path.join(self.embeddings_dir, \"chroma.sqlite3\")):\n",
    "            vector_store = Chroma(persist_directory=self.embeddings_dir, embedding_function=HuggingFaceEmbeddings(\n",
    "                model_name=self.model_name,\n",
    "                model_kwargs=self.model_kwargs,\n",
    "                encode_kwargs=self.encode_kwargs\n",
    "            ))\n",
    "            vector_store.delete()\n",
    "            print(\"All documents deleted successfully.\")\n",
    "        else:\n",
    "            print(f\"Vector store at {self.embeddings_dir} does not exist.\")\n",
    "\n",
    "    def getRetriever(self, vector_store: Chroma):\n",
    "        \"\"\"\n",
    "        Get a retriever from the vector store for similarity search.\n",
    "        \"\"\"\n",
    "        retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "        return retriever\n",
    "\n",
    "    def genPrompt(self, question: str, retriever: BaseRetriever):\n",
    "        \"\"\"\n",
    "        Generate a prompt based on the retrieved documents for the given question.\n",
    "        \"\"\"\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "        context = ' '.join([doc.page_content for doc in retrieved_docs])\n",
    "        prompt = f\"\"\"\n",
    "            [Instructions] \n",
    "                Question: {question}\n",
    "                Context: {context} \n",
    "                Answer:\n",
    "            [/Instructions]\n",
    "            \"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def loadContentFromPDF(self, pdf_path: str):\n",
    "        \"\"\"\n",
    "        Load content from a PDF file.\n",
    "        \"\"\"\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        docs = loader.load_and_split()\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3b4dbfbc-a389-4f2f-b21e-9c6373e06e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# นำเข้าโมดูลที่ต้องการ\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# วิธีใช้งาน\n",
    "# from src.utils.typhoon_2_assistant import Typhoon2Assistant\n",
    "# assistant = Typhoon2Assistant()\n",
    "# assistant.ask(\"ใส่ข้อความที่ต้องการถามที่นี่\")\n",
    "\n",
    "class Typhoon2Assistant:\n",
    "    def __init__(self):\n",
    "        self.model_id = \"scb10x/llama3.2-typhoon2-1b-instruct\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16)\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    def ask(self, prompt: str):\n",
    "        # เตรียมข้อความนำเข้า\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a friendly assistant. Answer the question based only on the following context. If you don't know the answer, then reply, No Context available for this question.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        # แปลงข้อความเป็น token (คืนค่าเป็น tensor โดยตรง)\n",
    "        input_ids = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # สร้าง attention_mask จาก input_ids (1 สำหรับ token จริง, 0 สำหรับ padding)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long().to(self.model.device)\n",
    "        \n",
    "        # กำหนดพารามิเตอร์สำหรับการสร้างข้อความ\n",
    "        terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
    "        \n",
    "        # สร้างข้อความตอบกลับ\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # ส่ง attention_mask เข้าไป\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=self.tokenizer.pad_token_id  # กำหนด pad_token_id อย่างชัดเจน\n",
    "        )\n",
    "        \n",
    "        # แปลงผลลัพธ์กลับเป็นข้อความ\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        answer = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "        return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "de362aee-c9b4-44fc-b160-7db34ee9b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AskWebsite:\n",
    "    def __init__(self,url: str,targetClassName: str, clearOldData: bool = False):\n",
    "        self.url = url\n",
    "        self.targetClassName = targetClassName\n",
    "        self.regUtils = RagUtils(embeddings_dir=\"./embeddings-load-website\")\n",
    "\n",
    "        if clearOldData:\n",
    "            self.regUtils.deleteAllDocuments()\n",
    "        \n",
    "        self.ingest();\n",
    "             \n",
    "    def ingest(self):\n",
    "        documents = self.regUtils.loadContentFromWebsite(\n",
    "            url=self.url,\n",
    "            targetClassName=self.targetClassName\n",
    "        )\n",
    "        chunks = self.regUtils.documentsSplitter(\n",
    "            documents=documents, \n",
    "            chunk_size=512, \n",
    "            chunk_overlap=51\n",
    "        )\n",
    "        self.regUtils.ingest(\n",
    "            chunks=chunks,\n",
    "        )\n",
    "        vectorstore = self.regUtils.loadVectorStore()\n",
    "        retriever = self.regUtils.getRetriever(vectorstore)\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        prompt = self.regUtils.genPrompt(question=question, retriever=self.retriever)\n",
    "        llm = Typhoon2Assistant()\n",
    "        answer = llm.ask(prompt)\n",
    "        return answer, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "824944e0-9733-4b70-a79c-1682e8eaf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AskPDF:\n",
    "    def __init__(self, pdf_path: str, clearOldData: bool = False):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.regUtils = RagUtils(embeddings_dir=\"./embeddings-load-pdf\")\n",
    "\n",
    "        if clearOldData:\n",
    "            self.regUtils.deleteAllDocuments()\n",
    "        \n",
    "        self.ingest();\n",
    "             \n",
    "    def ingest(self):\n",
    "        documents = self.regUtils.loadContentFromPDF(\n",
    "            pdf_path=self.pdf_path,\n",
    "        )\n",
    "        chunks = self.regUtils.documentsSplitter(\n",
    "            documents=documents, \n",
    "            chunk_size=512, \n",
    "            chunk_overlap=51\n",
    "        )\n",
    "        self.regUtils.ingest(\n",
    "            chunks=chunks,\n",
    "        )\n",
    "        vectorstore = self.regUtils.loadVectorStore()\n",
    "        retriever = self.regUtils.getRetriever(vectorstore)\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        prompt = self.regUtils.genPrompt(question=question, retriever=self.retriever)\n",
    "        llm = Typhoon2Assistant()\n",
    "        answer = llm.ask(prompt)\n",
    "        return answer, prompt\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "beb1f325-169d-4202-a426-660a50433355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 268 chunks.\n",
      "-- use New vector store--\n",
      "-- Ingest to Vector Database Success ---\n",
      "-- Vector Store Loaded --\n"
     ]
    }
   ],
   "source": [
    "assistance = AskPDF(\n",
    "    pdf_path=\"doc1.pdf\",\n",
    "    clearOldData=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fdb5d0f4-fb33-49f5-b30c-0e226232fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            [Instructions] \n",
      "                Question: เป็นงานวิจัยเกี่ยวกับอะไร\n",
      "                Context: 1.5 ขอบเขตของการวิจัย 2 \n",
      " 1.6 ประโยชน์ที่คาดว่าจะได้รับ 3 \n",
      " 1.7 นิยามศัพท์เฉพาะ 3 \n",
      "2 เอกสารและงานวิจัยที่เกี่ยวข้อง 4 \n",
      " 2.1 เซลลูโลส (cellulose) 4 \n",
      " 2.2 โครงสร้างของเซลลูโลส 5 \n",
      " 2.3 ชนิดของเซลลูโลส  5 \n",
      " 2.4 เทคโนโลยี Pre-treatment สำหรับวัสดุที่มีเซลลูโลสสูง 6 \n",
      " 2.5 คาร์บอกซีเมทิลเซลลูโลส    9 \n",
      " 2.6 จุลินทรีย์ที่สร้างเอนไซม์เซลลูเลส 10 \n",
      " 2.7 แบคทีเรียเซลลูเลส 11 \n",
      " 2.8 เอนไซม์เซลลูเลส (cellulase) 12 \n",
      " 2.9 การทำงานของเอนไซม์เซลลูเลส 14 \n",
      " 2.10 การสังเคราะห์เอนไซม์เซลลูเลส 15 ATGGCAACTAAAATCAAGGGTTGCGCTCTTTGCGGGACTTAACCCAACTTCTCACAACAC\n",
      "GACCTGACAACAACCTTGCACCACCTGTCACTCTGCTCCCGAAGGAGAAGCCCTATCTCT\n",
      "AGGGTTGTCAAAGGATGTCAAGACCTGTTAAGGTTCTTCGCGTTGCTTCAAATTAAACCA\n",
      "CTTGCTCCACCGCTTGTGCGGGCCCCCTTCATTTCCTTTGATTTTCACCCTTGCGGCCGT\n",
      "ACTCCCCAGGCGGATTGCTTAATGCTTTAACTTCACCACTAAAGGGCGAAAACCCTCTAA\n",
      "CACTTAGCACTCTTCTTTTACGGCGTGAACTACCAGGGTATCTAATCCTGTTTGCTCCCC\n",
      "เอกสารฉบับนี้ดาวน์โหลดเมื่อวันที่ 26/02/2025\n",
      "โดย Choonaja Shaboo\n",
      "จากระบบคลังข้อมูลงานวิจัยไทย (TNRR) โดย Choonaja Shaboo\n",
      "จากระบบคลังข้อมูลงานวิจัยไทย (TNRR) \n",
      "                Answer:\n",
      "            [/Instructions]\n",
      "            \n",
      "-------\n",
      "เป็นงานวิจัยเกี่ยวกับกระบวนการสร้างเซลลูโลสในพืช\n"
     ]
    }
   ],
   "source": [
    "answer,prompt = assistance.ask(\"เป็นงานวิจัยเกี่ยวกับอะไร\")\n",
    "print(prompt)\n",
    "print(\"-------\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a209c29e-23e3-4528-80eb-345151cf95bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 8 chunks.\n",
      "-- use Append data to vector store--\n",
      "-- Ingest to Vector Database Success ---\n",
      "-- Vector Store Loaded --\n"
     ]
    }
   ],
   "source": [
    "assistance = AskWebsite(\n",
    "    url=\"https://www.sanook.com/travel/1451675/\",\n",
    "    targetClassName= \"EntryReaderInner\",\n",
    "    clearOldData=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bf44a588-ad7a-4a5c-a21f-a18a7d112e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            [Instructions] \n",
      "                Question: มีฟาสปอร์ตดีใหม\n",
      "                Context: ขั้นตอนการต่ออายุพาสปอร์ต \n",
      "ถ้าพาสปอร์ตของคุณใกล้หมดอายุ ไม่ต้องกังวล! เพราะตอนนี้การต่อพาสปอร์ตทำได้ง่ายและสะดวกมากขึ้น ไม่ต้องเสียเวลาลางานหรือรีบตื่นเช้าเหมือนเมื่อก่อน แถมยังมีบริการในวันเสาร์อีกด้วย\n",
      "1. การจองคิวต่ออายุพาสปอร์ตผ่านระบบออนไลน์\n",
      "เพื่อความสะดวกและลดระยะเวลารอคอย ควรจองคิวล่วงหน้าผ่านเว็บไซต์ www.qpassport.in.th โดยมีขั้นตอนดังนี้: ขั้นตอนการต่ออายุพาสปอร์ต \n",
      "ถ้าพาสปอร์ตของคุณใกล้หมดอายุ ไม่ต้องกังวล! เพราะตอนนี้การต่อพาสปอร์ตทำได้ง่ายและสะดวกมากขึ้น ไม่ต้องเสียเวลาลางานหรือรีบตื่นเช้าเหมือนเมื่อก่อน แถมยังมีบริการในวันเสาร์อีกด้วย\n",
      "1. การจองคิวต่ออายุพาสปอร์ตผ่านระบบออนไลน์\n",
      "เพื่อความสะดวกและลดระยะเวลารอคอย ควรจองคิวล่วงหน้าผ่านเว็บไซต์ www.qpassport.in.th โดยมีขั้นตอนดังนี้: ขั้นตอนการต่ออายุพาสปอร์ต \n",
      "ถ้าพาสปอร์ตของคุณใกล้หมดอายุ ไม่ต้องกังวล! เพราะตอนนี้การต่อพาสปอร์ตทำได้ง่ายและสะดวกมากขึ้น ไม่ต้องเสียเวลาลางานหรือรีบตื่นเช้าเหมือนเมื่อก่อน แถมยังมีบริการในวันเสาร์อีกด้วย\n",
      "1. การจองคิวต่ออายุพาสปอร์ตผ่านระบบออนไลน์\n",
      "เพื่อความสะดวกและลดระยะเวลารอคอย ควรจองคิวล่วงหน้าผ่านเว็บไซต์ www.qpassport.in.th โดยมีขั้นตอนดังนี้: \n",
      "                Answer:\n",
      "            [/Instructions]\n",
      "            \n",
      "คุณสามารถจองคิวต่ออายุพาสปอร์ตผ่านระบบออนไลน์ได้โดยเข้าไปที่เว็บไซต์ www.qpassport.in.th แล้วเลือก “จองคิวต่ออายุพาสปอร์ต” จากนั้นกรอกข้อมูลส่วนตัวและหมายเลขพาสปอร์ตของคุณให้ครบถ้วน จากนั้นสามารถเลือกวันและเวลาที่ต้องการให้เจ้าหน้าที่โทรติดต่อคุณเพื่อทำการนัดหมายในวันถัดไป\n"
     ]
    }
   ],
   "source": [
    "answer,prompt = assistance.ask(\"มีฟาสปอร์ตดีใหม\")\n",
    "print(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd08f4a-a1c6-4bc6-8ce7-b7ac25128ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
